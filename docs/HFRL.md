### 第一部分：PPO (Proximal Policy Optimization)

#### 阶段一：SFT (监督微调) —— “打基础”

- **目标**：让模型学会“讲人话”，具备基本的指令遵循能力。
- **做法**：标准的 Next Token Prediction（和你做微调一样）。
- **产出**：**SFT 模型**（作为后续 PPO 的起点）。

#### 阶段二：RM (奖励模型训练) —— “练考官”

- **目标**：训练一个能模拟人类偏好的打分器。
- **数据**：`(Prompt, 胜出回答, 落败回答)` 的成对数据。
- **产出**：**Reward Model (RM)**。输入一个回答，它输出一个标量分数（比如 7.5 分）。

#### 阶段三：PPO (强化学习) —— “刷题提分” （核心难点）

| **模型角色**  | **学名**     | **状态**           | **作用**                                                     |
| ------------- | ------------ | ------------------ | ------------------------------------------------------------ |
| **Actor**     | Policy Model | **训练中 (Train)** | **学生**。生成回答，我们要的就是它。                         |
| **Critic**    | Value Model  | **训练中 (Train)** | **教练**。预测得分，用来计算“优势 (Advantage)”，帮助学生更稳地进步。 |
| **Reward**    | Reward Model | **冻结 (Freeze)**  | **考官**。提供 Ground Truth 的分数，这是训练的目标指引。     |
| **Reference** | Ref Model    | **冻结 (Freeze)**  | **班主任**。盯着学生别让他为了高分“走火入魔”（计算 KL 散度约束）。 |

这是一个不断循环的过程（Rollout -> Evaluate -> Update）：

**1. 采样 (Rollout):**

- **Actor (学生/当前模型)** 根据 Prompt 生成一个回答。

**2. 评估 (Evaluation):**

- **Reward Model (考官)** 给这个回答打**最终分** (Reward)。
- **Critic (教练)** 同时也看这道题，预测学生**原本应该得多少分** (Value)。
- **Reference Model (班主任)** 对比学生的回答和 SFT 原始回答的差异，计算 **KL 散度**（防止跑题）。

**3. 优势计算 (Advantage Calculation):**

- 计算 **Advantage (优势)** = `实际得分(Reward)` - `预测得分(Value)`。
  - 如果结果是正的，说明表现**超常发挥**（要奖励）。
  - 如果结果是负的，说明表现**失常**（要惩罚）。
- *(专业术语加分项：这里通常使用 **GAE (Generalized Advantage Estimation)** 算法来平衡偏差和方差)*

**4. 参数更新 (Optimization):**

- **更新 Actor**：根据 Advantage 调整参数，让它下次多生成高分回答，少生成低分回答。
- **更新 Critic**：根据预测误差（实际分和预测分的差距），调整 Critic 让它下次估分更准。
- *Reward Model 和 Reference Model 全程冻结，不更新。*

------

### 第二部分：GRPO (Group Relative Policy Optimization)

#### 1. 它是干嘛的？

PPO 最大的痛点是**太贵了**。正如前面所说，PPO 内存里要存 **Critic** 模型。如果模型是 70B 的，那个 Critic 也是 70B 的，显存直接爆炸。

**GRPO (DeepSeek 的创新)** 问了一个问题：**我们真的需要单独养一个 Critic (教练) 来预测分数吗？**

#### 2. GRPO 的逻辑

> “GRPO 放弃了 Critic 模型，用**‘小组内部竞争’**来替代‘教练预测’。
>
> 它的做法是：对于同一个 Prompt，让模型一次性生成一组回答（比如 64 个）。
>
> 然后算出这 64 个回答的平均分（Baseline）。
>
> - 比平均分高的，就是好回答（优势是正的）。
> - 比平均分低的，就是差回答（优势是负的）。
>
> **好处：** 省掉了一个巨型的 Critic 模型，显存占用大幅降低，训练速度更快。DeepSeek-R1 就是靠这个大大降低了强化学习的成本。”

### 第三部分：总结

1. **冷启动数据**：R1 在早期使用纯强化学习（Cold Start RL），不给任何 SFT 数据，让模型自己去试错，通过 GRPO 算法去探索思维链（CoT）。
2. **GRPO 算法**：它改进了传统的 PPO，去掉了 Critic 模型，通过对一组输出取平均值作为基准（Baseline），极大地降低了训练成本，让大规模强化学习成为可能。
3. **结果**：这种纯 RL 的方式让模型“涌现”出了自我反思和长思维链的能力。
4. **我的项目**：虽然我不需要复现 GRPO（那是基座厂商的事），但我们利用了 R1 蒸馏出来的思维能力（Teacher-Student）来增强我垂直领域红娘 Agent 的逻辑抽取能力，这是一种“站在巨人肩膀上”的应用层创新。